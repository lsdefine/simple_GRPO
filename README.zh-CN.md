# 🚀🚀🚀 simple_GRPO 🚀🚀🚀

一个简单的GRPO实现，用于复现类r1的LLM思考能力。本项目是一个简单的开源实现，使用了来自Hugging Face的trl的核心损失计算公式。

我们构建了最简单的代码库，支持：
- 节省GPU内存，实现可行且高效的训练
- 从教学角度快速理解GRPO等RL过程
- 快速尝试多种改进，如多答案生成、重组、KL惩罚和参数调优
- 在模型训练早期观察到"顿悟时刻"

## ✨最新更新
- 2025/02/19: 添加了loss triton实现，略有加速，可选择不使用。详见*simple_grpo_v1*文件夹
- 2025/02/19: 添加了重组版本，在ref_server上实现了生成数据采样。详见*regroup_ver*文件夹
- 2025/02/27: 添加了vllm包以加速推理
- 2025/03/24: 添加了reinforce++算法，使用方式与之前相同

## 🌟 特点

### 💡 简洁性
项目代码简单，仅有约200行代码分布在2个文件中。只依赖_deepspeed_和_torch_等标准库，无需依赖ray等。设计支持更复杂的干预。

### 🤖 拆分的参考模型
参考模型部分被解耦，允许在不同GPU上运行（甚至可在配有4090的不同机器上运行）。避免了将参考模型和训练模型放在同一GPU上，防止torch多进程创建多个副本，实现在80G A800上训练7B模型。

### 💃 性能
在1台A800 GPU上训练不到1小时完成。Qwen2.5-7B和Qwen2.5-3B均在前30步优化中出现"顿悟时刻"。

### 🥳 核心损失计算
损失计算公式基于Hugging Face的trl。感谢Hugging Face的贡献。

## 🙌 环境
运行环境在requirements.txt中，执行：
```bash
pip install -r requirements.txt
```
至少需要两个GPU。

## 使用方法
### 如果有三个或更多GPU，您将有更好的选择！
运行以下命令：
```bash
CUDA_VISIBLE_DEVICES=7 python ref_server.py
```
这仅使用一个GPU来收集并运行参考模型。

在*grpo_vllm_one.py*中，设置相对于下一步可见设备的生成设备索引：
```bash
gen_device = 1
```
然后，打开另一个bash：
```bash
CUDA_VISIBLE_DEVICES=2,3,4,5,6 deepspeed grpo_vllm_one.py
```

## ✨ 实验结果

1. 运行环境
- 硬件设置：2×A800 (80GB) GPU
- 配置：
  - 训练：1个GPU，采用Zero-Stage 2优化
  - 推理：专用1个GPU（兼容3090/4090）

2. 训练性能
   
| 模型        | 步数 | 时间       |
|------------|------|------------|
| Qwen2.5-3B | 60   | 12分34秒   |
| Qwen2.5-7B | 60   | 16分40秒   |

2.1 Qwen2.5-3B
- 准确率：
  - 5步优化后稳定在60%以上
  - 峰值约70%
- 格式合规性：
  - 30步后达到约100%

2.2 Qwen2.5-7B
- 准确率：
  - 整个训练过程中保持>90%
- 格式合规性：
  - 30步内达到100%
- 收敛性：两项指标均显示快速收敛

## 顿悟时刻
1. Qwen2.5-3B（第20步）模型在思考过程中意识到初始计算错误，开始重新考虑问题。

2. Qwen2.5-7B（第20步）模型发现计算结果与总数不符，自我校正并重新评估。

## 😊 待办事项
- 答案生成可能无效，因为一个组可能包含全部错误答案或全部正确答案。需要组重组和更好的答案生成
- 如果生成长思考链(CoT)，GPU内存仍然紧张。必须分割组以使批次更小

我们已经实现并正在测试这些功能，很快将推出。 